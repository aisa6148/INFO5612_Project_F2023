{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "# Temporary folder for data we need during execution of this notebook (we'll clean up\n",
    "# at the end, we promise)\n",
    "temp_dir = os.path.join(tempfile.gettempdir(), 'mind')\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# The dataset is split into training and validation set, each with a large and small version.\n",
    "# The format of the four files are the same.\n",
    "# For demonstration purpose, we will use small version validation set only.\n",
    "base_url = 'https://mind201910small.blob.core.windows.net/release'\n",
    "training_small_url = f'{base_url}/MINDsmall_train.zip'\n",
    "validation_small_url = f'{base_url}/MINDsmall_dev.zip'\n",
    "training_large_url = f'{base_url}/MINDlarge_train.zip'\n",
    "validation_large_url = f'{base_url}/MINDlarge_dev.zip'\n",
    "\n",
    "def download_url(url,\n",
    "                 destination_filename=None,\n",
    "                 progress_updater=None,\n",
    "                 force_download=False,\n",
    "                 verbose=True):\n",
    "    \"\"\"\n",
    "    Download a URL to a temporary file\n",
    "    \"\"\"\n",
    "    if not verbose:\n",
    "        progress_updater = None\n",
    "    # This is not intended to guarantee uniqueness, we just know it happens to guarantee\n",
    "    # uniqueness for this application.\n",
    "    if destination_filename is None:\n",
    "        url_as_filename = url.replace('://', '_').replace('/', '_')\n",
    "        destination_filename = \\\n",
    "            os.path.join(temp_dir,url_as_filename)\n",
    "    if (not force_download) and (os.path.isfile(destination_filename)):\n",
    "        if verbose:\n",
    "            print('Bypassing download of already-downloaded file {}'.format(\n",
    "                os.path.basename(url)))\n",
    "        return destination_filename\n",
    "    if verbose:\n",
    "        print('Downloading file {} to {}'.format(os.path.basename(url),\n",
    "                                                 destination_filename),\n",
    "              end='')\n",
    "    urllib.request.urlretrieve(url, destination_filename, progress_updater)\n",
    "    assert (os.path.isfile(destination_filename))\n",
    "    nBytes = os.path.getsize(destination_filename)\n",
    "    if verbose:\n",
    "        print('...done, {} bytes.'.format(nBytes))\n",
    "    return destination_filename\n",
    "\n",
    "zip_path = download_url(validation_small_url, verbose=True)\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(temp_dir)\n",
    "\n",
    "os.listdir(temp_dir)\n",
    "\n",
    "news_path = os.path.join(temp_dir, 'news.tsv')\n",
    "news_df = pd.read_table(news_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'id', 'category', 'subcategory', 'title', 'abstract', 'url',\n",
    "                  'title_entities', 'abstract_entities'\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_similarity(nid_1, nid_2):\n",
    "    if nid_1 not in news_df.index or nid_2 not in news_df.index:\n",
    "        return 0\n",
    "    \n",
    "    cat1 = news_df.loc[nid_1][\"category\"]\n",
    "    cat2 = news_df.loc[nid_2][\"category\"]\n",
    "    \n",
    "    return 1 - torch.cosine_similarity(glove[cat1].unsqueeze(0), glove[cat2].unsqueeze(0)).item()\n",
    "\n",
    "\n",
    "def diversity_user(recs):\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for i in range(len(recs)):\n",
    "        for j in range(i+1, len(recs)):\n",
    "            count += 1\n",
    "            score += get_category_similarity(recs[i], recs[j])\n",
    "    return score/count\n",
    "        \n",
    "# Calculates recommendations according to mmr for all users\n",
    "# df is a Pandas dataframe with cols user, news_id, pred where pred[i] is the relevance score for news_id[i]\n",
    "# lamda is a weight parameter\n",
    "# k is how many items should be in the recommendation; assume k >= 1\n",
    "def diversity_eval(df):\n",
    "    diversity_score = 0\n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        diversity_score += diversity_user(row['news_id'])\n",
    "        count += 1\n",
    "    return diversity_score/count"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ec2189bea0434770dca7423a25e631e1cca9c4e2b4ff137a82f4dff32ac9607"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
