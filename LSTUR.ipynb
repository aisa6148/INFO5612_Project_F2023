{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTUR: Neural News Recommendation with Long- and Short-term User Representations\n",
    "LSTUR \\[1\\] is a news recommendation approach capturing users' both long-term preferences and short-term interests. The core of LSTUR is a news encoder and a user encoder.  In the news encoder, we learn representations of news from their titles. In user encoder, we propose to learn long-term\n",
    "user representations from the embeddings of their IDs. In addition, we propose to learn short-term user representations from their recently browsed news via GRU network. Besides, we propose two methods to combine\n",
    "long-term and short-term user representations. The first one is using the long-term user representation to initialize the hidden state of the GRU network in short-term user representation. The second one is concatenating both\n",
    "long- and short-term user representations as a unified user vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]\n",
      "Tensorflow version: 2.14.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.lstur import LSTURModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import MMR\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17.0k/17.0k [00:01<00:00, 9.21kKB/s]\n",
      "100%|██████████| 9.84k/9.84k [00:01<00:00, 5.85kKB/s]\n",
      "100%|██████████| 95.0k/95.0k [00:27<00:00, 3.47kKB/s]\n"
     ]
    }
   ],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "data_path = tmpdir.name\n",
    "# Options: demo, small, large\n",
    "MIND_type = 'demo'\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\", r'lstur.yaml')\n",
    "\n",
    "mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_type)\n",
    "\n",
    "if not os.path.exists(train_news_file):\n",
    "    download_deeprec_resources(mind_url, os.path.join(data_path, 'train'), mind_train_dataset)\n",
    "    \n",
    "if not os.path.exists(valid_news_file):\n",
    "    download_deeprec_resources(mind_url, \\\n",
    "                               os.path.join(data_path, 'valid'), mind_dev_dataset)\n",
    "if not os.path.exists(yaml_file):\n",
    "    download_deeprec_resources(r'https://recodatasets.z20.web.core.windows.net/newsrec/', \\\n",
    "                               os.path.join(data_path, 'utils'), mind_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "seed = 40\n",
    "batch_size = 32\n",
    "\n",
    "def train_model():\n",
    "    hparams = prepare_hparams(yaml_file, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs)\n",
    "    print(hparams)\n",
    "    model = LSTURModel(hparams, MINDIterator, seed=seed)\n",
    "    print(model.run_eval(valid_news_file, valid_behaviors_file))\n",
    "    model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HParams object with values {'support_quick_scoring': True, 'dropout': 0.2, 'attention_hidden_dim': 200, 'head_num': 4, 'head_dim': 100, 'filter_num': 400, 'window_size': 3, 'vert_emb_dim': 100, 'subvert_emb_dim': 100, 'gru_unit': 400, 'type': 'ini', 'user_emb_dim': 50, 'learning_rate': 0.0001, 'optimizer': 'adam', 'epochs': 5, 'batch_size': 32, 'show_step': 100000, 'title_size': 30, 'his_size': 50, 'data_format': 'news', 'npratio': 4, 'metrics': ['group_auc', 'mean_mrr', 'ndcg@5;10'], 'word_emb_dim': 300, 'cnn_activation': 'relu', 'model_type': 'lstur', 'loss': 'cross_entropy_loss', 'wordEmb_file': 'C:\\\\Users\\\\Eliza\\\\AppData\\\\Local\\\\Temp\\\\tmpjylb0wsv\\\\utils\\\\embedding.npy', 'wordDict_file': 'C:\\\\Users\\\\Eliza\\\\AppData\\\\Local\\\\Temp\\\\tmpjylb0wsv\\\\utils\\\\word_dict.pkl', 'userDict_file': 'C:\\\\Users\\\\Eliza\\\\AppData\\\\Local\\\\Temp\\\\tmpjylb0wsv\\\\utils\\\\uid2index.pkl'}\n",
      "Tensor(\"conv1d/Relu:0\", shape=(None, 30, 400), dtype=float32)\n",
      "Tensor(\"att_layer2/Sum_1:0\", shape=(None, 400), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eliza\\anaconda3\\envs\\info_5612_project\\lib\\site-packages\\keras\\src\\optimizers\\legacy\\adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "0it [00:00, ?it/s]c:\\Users\\Eliza\\anaconda3\\envs\\info_5612_project\\lib\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "586it [00:08, 72.63it/s] \n",
      "4it [00:02,  1.87it/s]"
     ]
    }
   ],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "    print(res_syn)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, data_path):\n",
    "    model_path = os.path.join(data_path, \"model\")\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model.model.save_weights(os.path.join(model_path, \"lstur_ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(model, data_path):\n",
    "    group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)\n",
    "    with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
    "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')\n",
    "\n",
    "    f = zipfile.ZipFile(os.path.join(data_path, 'prediction.zip'), 'w', zipfile.ZIP_DEFLATED)\n",
    "    f.write(os.path.join(data_path, 'prediction.txt'), arcname='prediction.txt')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    user_ids = []\n",
    "    news_rec_lists = []\n",
    "    pred_prob = []\n",
    "    i = 0\n",
    "    group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)\n",
    "    with open(valid_behaviors_file, 'r') as rd:\n",
    "            impr_index = 0\n",
    "            for line in rd:\n",
    "                uid, time, history, impr = line.strip(\"\\n\").split('\\t')[-4:]\n",
    "\n",
    "                impr_news = [i.split(\"-\")[0] for i in impr.split()]\n",
    "                user_ids.append(uid)\n",
    "                news_rec_lists.append(impr_news)\n",
    "                pred_prob.append(group_preds[i])\n",
    "                i+=1\n",
    "    user_rec_df = pd.DataFrame({'user_id' : user_ids, 'news_id': news_rec_lists, 'pred' : pred_prob})\n",
    "    user_rec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_array(arr):\n",
    "    return (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n",
    "    \n",
    "def normalize_df_for_mmr(df):\n",
    "    df_normalized = df.copy()\n",
    "    df_normalized['pred'] = df_normalized['pred'].apply(lambda x: normalize_array(x))\n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df()\n",
    "news_df = MMR.get_news_df()\n",
    "glove = MMR.load_glove()\n",
    "k = 5\n",
    "\n",
    "df_at_k = df.copy()\n",
    "df_at_k[\"news_id\"] = df[\"news_id\"].apply(lambda x: x[:k])\n",
    "\n",
    "print(f\"NDCG@{k} (baseline): {evaluation.calculate_ndcg_at_k(df_at_k, k)}\")\n",
    "print(f\"Diversity (baseline): {evaluation.diversity_eval(glove, news_df, df_at_k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After re-ranking via MMR\n",
    "Note that lamda = 0 means all diversity, no relevance, and lamda = 1 means all relevance, no diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamdas = [x/100.0 for x in range(0, 125, 25)]\n",
    "diversities = []\n",
    "ndcgs = []\n",
    "exploded_df = df.copy().reset_index().explode(['pred', 'label', 'news_id']) #split back into columns\n",
    "normalized_df = normalize_df_for_mmr(df)\n",
    "for i in lamdas:\n",
    "    print(f\"\\nReranking with lambda={i}...\")\n",
    "    mmr_rerank_data = MMR.mmr_all(glove, news_df, normalized_df, i, k)\n",
    "    mmr_rerank_df = pd.DataFrame.from_dict(mmr_rerank_data, orient=\"index\").reset_index()\n",
    "\n",
    "    diversity = evaluation.diversity_eval(glove, news_df, mmr_rerank_df)\n",
    "    print(f\"Diversity: {diversity}\")\n",
    "    diversities.append(diversity)\n",
    "\n",
    "    mmr_rerank_df = mmr_rerank_df.rename({\"index\": \"user_id\"}, axis=1)\n",
    "    split_df = mmr_rerank_df.set_index([\"user_id\"]).apply(lambda x: x.explode()).reset_index()\n",
    "    split_df = split_df.rename({\"pred\": \"mmr_pred\"}, axis=1)\n",
    "    mmr_labels = pd.merge(exploded_df, split_df, on=[\"user_id\", \"news_id\"], how=\"right\")\n",
    "    mmr_labels_lists = mmr_labels.groupby(\"user_id\").agg({\"label\": list, \"mmr_pred\": list})\n",
    "    mmr_labels_lists.rename(columns={\"mmr_pred\": \"pred\"}, inplace=True)\n",
    "    ndcg = evaluation.calculate_ndcg_at_k(mmr_labels_lists, k)\n",
    "    print(f\"NDCG@{k}: {ndcg}\")\n",
    "    ndcgs.append(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.graph_ndcg(ndcgs, lamdas, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.graph_diversity(diversities, lamdas)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "3a9a0c422ff9f08d62211b9648017c63b0a26d2c935edc37ebb8453675d13bb5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
