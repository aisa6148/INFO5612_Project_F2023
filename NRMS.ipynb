{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRMS: Neural News Recommendation with Multi-Head Self-Attention\n",
    "NRMS \\[1\\] is a neural news recommendation approach with multi-head selfattention. The core of NRMS is a news encoder and a user encoder. In the newsencoder, a multi-head self-attentions is used to learn news representations from news titles by modeling the interactions between words. In the user encoder, we learn representations of users from their browsed news and use multihead self-attention to capture the relatedness between the news. Besides, we apply additive\n",
    "attention to learn more informative news and user representations by selecting important words and news.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]\n",
      "Tensorflow version: 2.14.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.nrms import NRMSModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17.0k/17.0k [00:01<00:00, 9.75kKB/s]\n",
      "100%|██████████| 9.84k/9.84k [00:01<00:00, 5.49kKB/s]\n",
      "100%|██████████| 95.0k/95.0k [00:06<00:00, 14.1kKB/s]\n"
     ]
    }
   ],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "data_path = tmpdir.name\n",
    "# Options: demo, small, large\n",
    "MIND_type = 'demo'\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\", r'nrms.yaml')\n",
    "\n",
    "mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_type)\n",
    "\n",
    "if not os.path.exists(train_news_file):\n",
    "    download_deeprec_resources(mind_url, os.path.join(data_path, 'train'), mind_train_dataset)\n",
    "    \n",
    "if not os.path.exists(valid_news_file):\n",
    "    download_deeprec_resources(mind_url, \\\n",
    "                               os.path.join(data_path, 'valid'), mind_dev_dataset)\n",
    "if not os.path.exists(yaml_file):\n",
    "    download_deeprec_resources(r'https://recodatasets.z20.web.core.windows.net/newsrec/', \\\n",
    "                               os.path.join(data_path, 'utils'), mind_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HParams object with values {'support_quick_scoring': True, 'dropout': 0.2, 'attention_hidden_dim': 200, 'head_num': 20, 'head_dim': 20, 'filter_num': 200, 'window_size': 3, 'vert_emb_dim': 100, 'subvert_emb_dim': 100, 'gru_unit': 400, 'type': 'ini', 'user_emb_dim': 50, 'learning_rate': 0.0001, 'optimizer': 'adam', 'epochs': 5, 'batch_size': 32, 'show_step': 10, 'title_size': 30, 'his_size': 50, 'data_format': 'news', 'npratio': 4, 'metrics': ['group_auc', 'mean_mrr', 'ndcg@5;10'], 'word_emb_dim': 300, 'model_type': 'nrms', 'loss': 'cross_entropy_loss', 'wordEmb_file': '/var/folders/sv/970s472x5tz75kxxbpmggty40000gn/T/tmp0rkqfvdu/utils/embedding.npy', 'wordDict_file': '/var/folders/sv/970s472x5tz75kxxbpmggty40000gn/T/tmp0rkqfvdu/utils/word_dict.pkl', 'userDict_file': '/var/folders/sv/970s472x5tz75kxxbpmggty40000gn/T/tmp0rkqfvdu/utils/uid2index.pkl'}\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "\n",
    "def train_model():\n",
    "    hparams = prepare_hparams(yaml_file, \n",
    "        wordEmb_file=wordEmb_file,\n",
    "        wordDict_file=wordDict_file, \n",
    "        userDict_file=userDict_file,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        show_step=10)\n",
    "    print(hparams)\n",
    "    model = NRMSModel(hparams, MINDIterator, seed=seed)\n",
    "    print(model.run_eval(valid_news_file, valid_behaviors_file))\n",
    "    model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "586it [00:06, 96.85it/s]\n",
      "236it [01:48,  2.18it/s]\n",
      "7538it [00:00, 15671.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.4792, 'mean_mrr': 0.2059, 'ndcg@5': 0.2045, 'ndcg@10': 0.2701}\n",
      "CPU times: user 9min 33s, sys: 6.29 s, total: 9min 39s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "def eval_model(model):\n",
    "    res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "    print(res_syn)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, data_path):\n",
    "    model_path = os.path.join(data_path, \"model\")\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model.model.save_weights(os.path.join(model_path, \"nrms_ckpt\"))\n",
    "\n",
    "def write_predictions(model, data_path):\n",
    "    group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)\n",
    "    with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
    "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')\n",
    "\n",
    "    f = zipfile.ZipFile(os.path.join(data_path, 'prediction.zip'), 'w', zipfile.ZIP_DEFLATED)\n",
    "    f.write(os.path.join(data_path, 'prediction.txt'), arcname='prediction.txt')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    user_ids = []\n",
    "    news_rec_lists = []\n",
    "    pred_prob = []\n",
    "    i = 0\n",
    "    group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)\n",
    "    with open(valid_behaviors_file, 'r') as rd:\n",
    "            impr_index = 0\n",
    "            for line in rd:\n",
    "                uid, time, history, impr = line.strip(\"\\n\").split('\\t')[-4:]\n",
    "\n",
    "                impr_news = [i.split(\"-\")[0] for i in impr.split()]\n",
    "                user_ids.append(uid)\n",
    "                news_rec_lists.append(impr_news)\n",
    "                pred_prob.append(group_preds[i])\n",
    "                i+=1\n",
    "    user_rec_df = pd.DataFrame({'user_id' : user_ids, 'news_id': news_rec_lists, 'pred' : pred_prob})\n",
    "    return user_rec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_array(arr):\n",
    "    return (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n",
    "    \n",
    "def normalize_df_for_mmr(df):\n",
    "    df_normalized = df.copy()\n",
    "    df_normalized['pred'] = df_normalized['pred'].apply(lambda x: normalize_array(x))\n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df()\n",
    "news_df = MMR.get_news_df()\n",
    "glove = MMR.load_glove()\n",
    "k = 5\n",
    "\n",
    "df_at_k = df.copy()\n",
    "df_at_k[\"news_id\"] = df[\"news_id\"].apply(lambda x: x[:k])\n",
    "\n",
    "print(f\"NDCG@{k} (baseline): {evaluation.calculate_ndcg_at_k(df_at_k, k)}\")\n",
    "print(f\"Diversity (baseline): {evaluation.diversity_eval(glove, news_df, df_at_k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After re-ranking via MMR\n",
    "Note that lamda = 0 means all diversity, no relevance, and lamda = 1 means all relevance, no diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamdas = [x/100.0 for x in range(0, 125, 25)]\n",
    "diversities = []\n",
    "ndcgs = []\n",
    "exploded_df = df.copy().reset_index().explode(['pred', 'label', 'news_id']) #split back into columns\n",
    "normalized_df = normalize_df_for_mmr(df)\n",
    "for i in lamdas:\n",
    "    print(f\"\\nReranking with lambda={i}...\")\n",
    "    mmr_rerank_data = MMR.mmr_all(glove, news_df, normalized_df, i, k)\n",
    "    mmr_rerank_df = pd.DataFrame.from_dict(mmr_rerank_data, orient=\"index\").reset_index()\n",
    "\n",
    "    diversity = evaluation.diversity_eval(glove, news_df, mmr_rerank_df)\n",
    "    print(f\"Diversity: {diversity}\")\n",
    "    diversities.append(diversity)\n",
    "\n",
    "    mmr_rerank_df = mmr_rerank_df.rename({\"index\": \"user_id\"}, axis=1)\n",
    "    split_df = mmr_rerank_df.set_index([\"user_id\"]).apply(lambda x: x.explode()).reset_index()\n",
    "    split_df = split_df.rename({\"pred\": \"mmr_pred\"}, axis=1)\n",
    "    mmr_labels = pd.merge(exploded_df, split_df, on=[\"user_id\", \"news_id\"], how=\"right\")\n",
    "    mmr_labels_lists = mmr_labels.groupby(\"user_id\").agg({\"label\": list, \"mmr_pred\": list})\n",
    "    mmr_labels_lists.rename(columns={\"mmr_pred\": \"pred\"}, inplace=True)\n",
    "    ndcg = evaluation.calculate_ndcg_at_k(mmr_labels_lists, k)\n",
    "    print(f\"NDCG@{k}: {ndcg}\")\n",
    "    ndcgs.append(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.graph_ndcg(ndcgs, lamdas, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.graph_diversity(diversities, lamdas)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "3a9a0c422ff9f08d62211b9648017c63b0a26d2c935edc37ebb8453675d13bb5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tf2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
